{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d943037d",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import tqdm\n",
    "import torch\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4b7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI escape codes for colors\n",
    "PINK = '\\033[95m'\n",
    "CYAN = '\\033[96m'\n",
    "YELLOW = '\\033[93m'\n",
    "NEON_GREEN = '\\033[92m'\n",
    "RESET_COLOR = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42636f58",
   "metadata": {},
   "source": [
    "## Collect Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57444e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingestion progress: 100%|██████████| 4/4 [00:00<00:00, 4009.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog content appended to 'blog_post.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def scrape_dfir_report(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"entry-content\", \"entry-title\", \"entry-header\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    dfir_content = \"\"\n",
    "    for split in tqdm.tqdm(splits, desc=\"Ingestion progress\"):\n",
    "        dfir_content += str(split)\n",
    "    dfir_content += \"\\n\"\n",
    "    return dfir_content\n",
    "\n",
    "def scrape_hacker_news(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"content section\", \"articlebody\", \"story-title\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    hn_content = \"\"\n",
    "    for split in tqdm.tqdm(splits, desc=\"Ingestion progress\"):\n",
    "        hn_content += str(split)\n",
    "    hn_content += \"\\n\"\n",
    "    return hn_content\n",
    "\n",
    "def scrape_bleeping_computer(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"article_section\", \"articleBody\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    bc_content = \"\"\n",
    "    for split in tqdm.tqdm(splits, desc=\"Ingestion progress\"):\n",
    "        bc_content += str(split)\n",
    "    bc_content += \"\\n\"\n",
    "    return bc_content\n",
    "\n",
    "def save_to_txt(content, filename):\n",
    "    with open(filename, 'a', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "while True:\n",
    "    option = input(\"Please choose an option:\\n1. Scrape DFIR Report\\n2. Scrape Hacker News\\n3. Scrape BleepingComputer\\nEnter 'quit' to complete the task: \")\n",
    "    \n",
    "    if option == 'quit':\n",
    "        break\n",
    "    \n",
    "    url = input(\"Please enter the URL: \")\n",
    "    \n",
    "    if option == '1':\n",
    "        # Scrape DFIR Report\n",
    "        blog_content = scrape_dfir_report(url)\n",
    "    elif option == '2':\n",
    "        # Scrape Hacker News\n",
    "        blog_content = scrape_hacker_news(url)\n",
    "    elif option == '3':\n",
    "        # Scrape BleepingComputer\n",
    "        blog_content = scrape_bleeping_computer(url)\n",
    "    else:\n",
    "        print(\"Invalid option. Please choose either 1, 2, or 3\")\n",
    "        continue\n",
    "    \n",
    "    # Save the content to a common file\n",
    "    if blog_content:\n",
    "        save_to_txt(blog_content, 'blog_post.txt')\n",
    "        print(\"Blog content appended to 'blog_post.txt'\")\n",
    "    else:\n",
    "        print(\"Failed to scrape blog content!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098a72a2",
   "metadata": {},
   "source": [
    "## Normalize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8134aa45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Splitting text into chunks: 100%|██████████| 543/543 [00:00<00:00, 356920.09it/s]\n",
      "Writing chunks to file: 100%|██████████| 148/148 [00:00<00:00, 148470.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mText file content appended to vault.txt with each chunk on a separate line. MD5 hash:\u001b[93m6716ecf284b84ce660f8fff35a50023f\u001b[0m\n",
      "\u001b[92mMD5 hash saved to vault.pid\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def upload_txtfile():\n",
    "    # Normalize whitespace and clean up text\n",
    "    with open(\"blog_post.txt\", \"r\", encoding=\"utf-8\") as vault_file:\n",
    "        text = vault_file.read()\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "        # Split text into chunks by sentences, respecting a maximum chunk size\n",
    "        sentences = re.split(r'(?<=[.!?]) +', text)  # split on spaces following sentence-ending punctuation\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "        for sentence in tqdm.tqdm(sentences, desc=\"Splitting text into chunks\"):\n",
    "            # Check if the current sentence plus the current chunk exceeds the limit\n",
    "            if len(current_chunk) + len(sentence) + 1 < 1000:  # +1 for the space\n",
    "                current_chunk += (sentence + \" \").strip()\n",
    "            else:\n",
    "                # When the chunk exceeds 1000 characters, store it and start a new one\n",
    "                chunks.append(current_chunk)\n",
    "                current_chunk = sentence + \" \"\n",
    "        if current_chunk:  # Don't forget the last chunk!\n",
    "            chunks.append(current_chunk)\n",
    "\n",
    "        # Write each chunk to its own line\n",
    "        with open(\"vault.txt\", \"w\", encoding=\"utf-8\") as vault_file:\n",
    "            for chunk in tqdm.tqdm(chunks, desc=\"Writing chunks to file\"):\n",
    "                vault_file.write(chunk.strip() + \"\\n\\n\")  # Two newlines to separate chunks\n",
    "\n",
    "    # Calculate the MD5 hash of the modified vault.txt file\n",
    "    md5_hash = hashlib.md5()\n",
    "    with open(\"vault.txt\", \"rb\") as vault_file:\n",
    "        for chunk in iter(lambda: vault_file.read(4096), b\"\"):\n",
    "            md5_hash.update(chunk)\n",
    "    md5_hash = md5_hash.hexdigest()\n",
    "\n",
    "    # Save the MD5 hash to a PID file\n",
    "    with open(\"vault.pid\", \"w\") as pid_file:\n",
    "        pid_file.write(md5_hash)\n",
    "\n",
    "    print(NEON_GREEN + \"Text file content appended to vault.txt with each chunk on a separate line. MD5 hash:\" + YELLOW + md5_hash + RESET_COLOR)\n",
    "    print(NEON_GREEN + \"MD5 hash saved to vault.pid\" + RESET_COLOR)\n",
    "\n",
    "# Call the upload_txtfile function\n",
    "upload_txtfile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01a5ec3",
   "metadata": {},
   "source": [
    "## Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8675af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mdeepseek-coder-v2:16b Response:\n",
      "\n",
      " {\n",
      "  \"IOCs\": [\n",
      "    {\n",
      "      \"Name\": \"config.cfg\",\n",
      "      \"File Size\": \"27392 Byte(s) SHA256: 28a9982cf2b4fc53a1545b6ed0d0c1788ca9369a847750f5652ffa0ca7f7b7d3\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"config.cfg\",\n",
      "      \"File Size\": \"28268 Byte(s) SHA256: 8afd6c0636c5d70ac0622396268786190a428635e9cf28ab23add939377727b0\"\n",
      "    },\n",
      "    {\n",
      "      \"Domain\": \"bunch-balance-councils[.]trycloudflare[.]com\",\n",
      "      \"Domain\": \"ferrari-rolling-facilities-lounge[.]trycloudflare[.]com\",\n",
      "      \"Domain\": \"galleries-physicians-psp-wv[.]trycloudflare[.]com\",\n",
      "      \"Domain\": \"evidence-deleted-procedure-bringing[.]trycloudflare[.]com\",\n",
      "      \"Domain\": \"nowhere-locked-manor-hs[.]trycloudflare[.]com\",\n",
      "      \"Domain\": \"ranked-accordingly-ab-hired[.]trycloudflare[.]com\"\n",
      "    },\n",
      "    {\n",
      "      \"IP\": \"64[.]95[.]12[.]71\",\n",
      "      \"IP\": \"184[.]95[.]51[.]165\"\n",
      "    }\n",
      "  ]\n",
      "}\u001b[0m\n",
      "\u001b[92mdeepseek-coder-v2:16b Response:\n",
      "\n",
      " To provide accurate help, I need to see the text you mentioned. Please share it so I can analyze and extract the most useful information for you.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Function to open a file and return its contents as a string\n",
    "def open_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as infile:\n",
    "        return infile.read()\n",
    "\n",
    "# Function to get relevant context from the vault based on user input\n",
    "def get_relevant_context(user_input, vault_embeddings, vault_content, model, top_k=3):\n",
    "    if vault_embeddings.nelement() == 0:  # Check if the tensor has any elements\n",
    "        return []\n",
    "    input_embedding = model.encode([user_input])\n",
    "    cos_scores = util.cos_sim(input_embedding, vault_embeddings)[0]\n",
    "    top_k = min(top_k, len(cos_scores))\n",
    "    top_indices = torch.topk(cos_scores, k=top_k)[1].tolist()\n",
    "    relevant_context = [vault_content[idx].strip() for idx in top_indices]\n",
    "    return relevant_context\n",
    "\n",
    "# Function to chat with the Ollama model\n",
    "def ollama_chat(user_input, system_message, vault_embeddings, vault_content, model, model_name=\"mistral\"):\n",
    "    relevant_context = get_relevant_context(user_input, vault_embeddings, vault_content, model)\n",
    "    \n",
    "    if relevant_context:\n",
    "        context_str = \"\\n\".join(relevant_context)\n",
    "        user_input_with_context = context_str + \"\\n\\n\" + user_input\n",
    "    else:\n",
    "        print(CYAN + \"No relevant context found.\" + RESET_COLOR)\n",
    "        user_input_with_context = user_input\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_input_with_context}\n",
    "    ]\n",
    "\n",
    "    response = requests.post(\n",
    "        \"http://localhost:11434/v1/chat/completions\",\n",
    "        json={\n",
    "            \"model\": model_name,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ollama Error: {response.status_code} - {response.text}\")\n",
    "    \n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "file_name = \"embeddings.pt\"\n",
    "vault_content = []\n",
    "\n",
    "# Function to create embeddings and save to disk\n",
    "def create_embeddings(vault_content, model):\n",
    "    vault_embeddings = model.encode(vault_content) if vault_content else []\n",
    "    vault_embeddings_tensor = torch.tensor(vault_embeddings)\n",
    "    save_embeddings(vault_embeddings_tensor)\n",
    "    return vault_embeddings_tensor\n",
    "\n",
    "# Save embeddings to a file\n",
    "def save_embeddings(embeddings):\n",
    "    torch.save(embeddings, file_name)\n",
    "\n",
    "# Calculate the hash of vault.pid\n",
    "with open(\"vault.pid\", \"rb\") as vault_file:\n",
    "    vault_hash = hashlib.sha256(vault_file.read()).hexdigest()\n",
    "\n",
    "# Check hash to decide whether to reuse or regenerate embeddings\n",
    "if os.path.exists(\"hash.pid\") and open(\"hash.pid\", \"r\").read() == vault_hash:\n",
    "    with open(\"vault.txt\", \"r\", encoding='utf-8') as vault_file:\n",
    "        vault_content = vault_file.readlines()\n",
    "    vault_embeddings_tensor = torch.load(file_name)\n",
    "else:\n",
    "    with open(\"vault.txt\", \"r\", encoding='utf-8') as vault_file:\n",
    "        vault_content = vault_file.readlines()\n",
    "        vault_embeddings_tensor = create_embeddings(vault_content, model)\n",
    "    with open(\"hash.pid\", \"w\") as hash_file:\n",
    "        hash_file.write(vault_hash)\n",
    "    torch.save(vault_embeddings_tensor, \"embeddings.pt\")\n",
    "\n",
    "# Main chat loop\n",
    "while True:\n",
    "    user_input = input(YELLOW + \"Ask a question about your documents, or type 'quit' to end the chat: \" + RESET_COLOR)\n",
    "    \n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "\n",
    "    system_message = \"You are a helpful assistant that is an expert at extracting the most useful information from a given text.\"\n",
    "    try:\n",
    "        response = ollama_chat(user_input, system_message, vault_embeddings_tensor, vault_content, model, model_name=\"deepseek-coder-v2:16b\")\n",
    "        print(NEON_GREEN + \"deepseek-coder-v2:16b Response:\\n\\n\" + response + RESET_COLOR)\n",
    "    except Exception as e:\n",
    "        print(PINK + f\"Error: {e}\" + RESET_COLOR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdd40de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
